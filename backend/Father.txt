import pandas as pd
import logging
import time
from concurrent.futures import ProcessPoolExecutor

# ------------ Utility to save DataFrame safely ------------
def save_feather_safe(df: pd.DataFrame, file_path: str):
    try:
        df.reset_index(drop=True).to_feather(file_path)
    except Exception as e:
        logging.error(f"Feather save failed for {file_path}, converting to string. Error: {e}")
        df = df.astype(str)
        df.reset_index(drop=True).to_feather(file_path)

# ------------ Worker Function ------------
def rec_rule(file_path: str, log_file_path: str):
    try:
        df = pd.read_feather(file_path)
    except Exception as e:
        logging.error(f"Feather read failed for {file_path}, converting to string. Error: {e}")
        df = pd.read_feather(file_path).astype(str)

    logging.error(f"Processing {file_path} with {len(df)} rows")
    # ðŸ‘‰ Do your heavy processing here
    return df.shape  # Example: return size

# ------------ Main Script ------------
if __name__ == "__main__":
    logging.basicConfig(level=logging.ERROR)

    # Example big dataframes (replace with your real ones)
    house_df = pd.DataFrame({
        "id": range(1_000_000),
        "val": [f"text_{i}" for i in range(1_000_000)]
    })
    detrep_df = pd.DataFrame({
        "id": range(1_000_000),
        "val": [i * 0.1 for i in range(1_000_000)]
    })

    t1 = time.time()

    # Save once to feather with fallback
    house_file = "house_df.feather"
    detrep_file = "detrep_df.feather"
    save_feather_safe(house_df, house_file)
    save_feather_safe(detrep_df, detrep_file)

    # Run multiprocessing without passing heavy DataFrames
    with ProcessPoolExecutor() as executor:
        futures = [
            executor.submit(rec_rule, house_file, "house_log.txt"),
            executor.submit(rec_rule, detrep_file, "detrep_log.txt")
        ]
        results = [f.result() for f in futures]

    t2 = time.time()
    logging.error(f"Multiprocessing done in {(t2-t1)/60:.2f} min")
    print("Results:", results)

import polars as pl

# Input
csv_file = "data.csv"
columns_to_read = ["col1", "col2", "col3"]   # ðŸ‘ˆ only these columns will be read
columns_to_combine = "col1,col2"             # ðŸ‘ˆ columns to merge
allowed_values = ["A_100", "B_200"]

# Step 1: Scan only selected columns, force Utf8
df = pl.scan_csv(csv_file, dtypes=pl.Utf8, columns=columns_to_read)

# Step 2: Split comma-separated combine list
cols = [c.strip() for c in columns_to_combine.split(",")]

# Step 3: Concatenate selected columns into new one
df = df.with_columns(
    pl.concat_str([pl.col(c) for c in cols], separator="_").alias("combined_col")
)
import polars as pl

# Example data
df1 = pl.DataFrame({"Col1": ["1,2,3"]})
df2 = pl.DataFrame({"Col1": ["1", "3"], "Col2": ["A", "C"]})

# Convert to LazyFrames
lf1 = df1.lazy()
lf2 = df2.lazy()

# Step 1: Split comma-separated strings into lists
lf1 = lf1.with_columns(
    pl.col("Col1").str.split(",").alias("Col1_list")
)

# Step 2: Explode so each value becomes its own row
lf1_exploded = lf1.explode("Col1_list")

# Step 3: Left join with df2
joined = (
    lf1_exploded.join(
        lf2,
        left_on="Col1_list",
        right_on="Col1",
        how="left"
    )
    .with_columns(
        pl.col("Col2").fill_null("Not found")  # handle missing matches
    )
)

# Step 4: Group back to original row structure
result = (
    joined.group_by("Col1")
    .agg([
        pl.col("Col2").str.concat(",").alias("Col2")
    ])
)

# Collect the result
final_df = result.collect()
print(final_df)
